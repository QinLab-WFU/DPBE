{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from stochman import nnj\n",
    "import numpy as np\n",
    "from jax import numpy as jnp\n",
    "from jax import vjp\n",
    "from jax import lax # for convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch_size = 1\n",
    "_input_size = 2\n",
    "_hidden_size = 48\n",
    "_hidden_c_in, _hidden_c_out = 3, 3\n",
    "_hidden_h, _hidden_w = 4, 4\n",
    "assert _hidden_size == _hidden_c_in * _hidden_h * _hidden_w\n",
    "_kernel_size, _padding = 3,1\n",
    "assert _kernel_size == 2*_padding + 1\n",
    "_output_size = 2\n",
    "\n",
    "\n",
    "x = torch.randn(_batch_size, _input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=48, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): ResidualBlock(\n",
       "    (_F): Sequential(\n",
       "      (0): Reshape()\n",
       "      (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): Flatten()\n",
       "    )\n",
       "  )\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=48, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_layers = [nnj.Linear(_input_size, _hidden_size),\n",
    "            nnj.Tanh()\n",
    "            ]\n",
    "B_layers = [nnj.Reshape(_hidden_c_in, _hidden_h, _hidden_w),\n",
    "            nnj.Conv2d(_hidden_c_in, _hidden_c_out, _kernel_size, stride=1, padding=_padding),\n",
    "            nnj.Flatten(),\n",
    "            ]\n",
    "C_layers = [nnj.Tanh(),\n",
    "            nnj.Linear(_hidden_size, _output_size),\n",
    "            #nnj.L2Norm()\n",
    "            ]\n",
    "                        \n",
    "stoch_model = nnj.Sequential(\n",
    "                    *A_layers,\n",
    "                    nnj.ResidualBlock(*B_layers, add_hooks = True),\n",
    "                    *C_layers,\n",
    "                    add_hooks=True\n",
    "                )\n",
    "\n",
    "stoch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner\n",
      " [torch.Size([1, 2]), torch.Size([1, 48]), torch.Size([1, 48]), torch.Size([1, 48]), torch.Size([1, 48]), torch.Size([1, 2])]\n",
      "outer\n",
      " [torch.Size([1, 48]), torch.Size([1, 3, 4, 4]), torch.Size([1, 3, 4, 4]), torch.Size([1, 48])]\n"
     ]
    }
   ],
   "source": [
    "### test single input ###\n",
    "y = stoch_model(x)\n",
    "\n",
    "print('inner\\n',[fm.shape for fm in stoch_model.feature_maps])\n",
    "print('outer\\n',[fm.shape for fm in stoch_model._modules_list[2]._F.feature_maps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reforwarding 6\n",
      "non reforwarding 4\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.ones(_batch_size, _output_size)\n",
    "GGN = stoch_model._jTmjp(x, None, matrix, wrt = 'input', from_diag = True, to_diag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "non reforwarding 4\n",
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "non reforwarding 4\n",
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "non reforwarding 4\n",
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "non reforwarding 4\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.ones(_batch_size, _output_size)\n",
    "GGN = stoch_model._jTmjp(x, None, matrix, wrt = 'input', from_diag = True, to_diag = True)\n",
    "GGN = stoch_model._jTmjp(x, None, matrix, wrt = 'input', from_diag = True, to_diag = False)\n",
    "GGN = stoch_model._jTmjp(x, None, matrix, wrt = 'weight', from_diag = True, to_diag = True)\n",
    "GGN = stoch_model._jTmjp(x, None, matrix, wrt = 'weight', from_diag = True, to_diag = False)\n",
    "\n",
    "matrix = torch.ones(_batch_size, _output_size, _output_size)\n",
    "GGN = stoch_model._jTmjp(x, None, matrix, wrt = 'input', from_diag = False, to_diag = True)\n",
    "GGN = stoch_model._jTmjp(x, None, matrix, wrt = 'input', from_diag = False, to_diag = False)\n",
    "GGN = stoch_model._jTmjp(x, None, matrix, wrt = 'weight', from_diag = False, to_diag = True)\n",
    "GGN = stoch_model._jTmjp(x, None, matrix, wrt = 'weight', from_diag = False, to_diag = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights in stoch model:\n",
      "\tlinear 1 - W\n",
      "\t (48, 2)\n",
      "\tlinear 1 - b\n",
      "\t (48,)\n",
      "\tconv - K\n",
      "\t (3, 3, 3, 3)\n",
      "\tconv - b\n",
      "\t (3,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tlinear 2 - W\n",
      "\t (2, 48)\n",
      "\tlinear 2 - b\n",
      "\t (2,)\n",
      "n weights per layer 144 84 98\n",
      "n weights total 326\n"
     ]
    }
   ],
   "source": [
    "first_linear_layer_index = 0\n",
    "resnet_index, conv_layer_index_in_resnet = 2, 1\n",
    "second_linear_layer_index = 4\n",
    "\n",
    "print('Weights in stoch model:')\n",
    "\n",
    "print('\\tlinear 1 - W\\n\\t',stoch_model._modules_list[first_linear_layer_index].weight.detach().numpy().shape)\n",
    "print('\\tlinear 1 - b\\n\\t',stoch_model._modules_list[first_linear_layer_index].bias.detach().numpy().shape)\n",
    "\n",
    "print('\\tconv - K\\n\\t',stoch_model._modules_list[resnet_index]._F._modules_list[conv_layer_index_in_resnet].weight.detach().numpy().shape)\n",
    "print('\\tconv - b\\n\\t',stoch_model._modules_list[resnet_index]._F._modules_list[conv_layer_index_in_resnet].bias.detach().numpy().shape)\n",
    "\n",
    "print('\\tlinear 2 - W\\n\\t',stoch_model._modules_list[second_linear_layer_index].weight.detach().numpy().shape)\n",
    "print('\\tlinear 2 - b\\n\\t',stoch_model._modules_list[second_linear_layer_index].bias.detach().numpy().shape)\n",
    "\n",
    "\n",
    "weights = []\n",
    "\n",
    "for row in stoch_model._modules_list[first_linear_layer_index].weight.detach().numpy():\n",
    "    weights = np.concatenate((weights, row))\n",
    "weights = np.concatenate((weights, stoch_model._modules_list[first_linear_layer_index].bias.detach().numpy()))\n",
    "first_linear_num_weights = len(weights)\n",
    "\n",
    "for c_out in range(_hidden_c_out):\n",
    "    for c_in in range(_hidden_c_in):\n",
    "        kernel = stoch_model._modules_list[resnet_index]._F._modules_list[conv_layer_index_in_resnet].weight.detach().numpy()[c_out,c_in]\n",
    "        for row in kernel:\n",
    "            weights = np.concatenate((weights, row))\n",
    "weights = np.concatenate((weights, stoch_model._modules_list[resnet_index]._F._modules_list[conv_layer_index_in_resnet].bias.detach().numpy()))\n",
    "conv_num_weights = len(weights) - first_linear_num_weights\n",
    "\n",
    "for row in stoch_model._modules_list[second_linear_layer_index].weight.detach().numpy():\n",
    "    weights = np.concatenate((weights, row))\n",
    "weights = np.concatenate((weights, stoch_model._modules_list[second_linear_layer_index].bias.detach().numpy()))\n",
    "second_linear_num_weights = len(weights) - first_linear_num_weights - conv_num_weights\n",
    "\n",
    "weights = jnp.array(weights)\n",
    "print('n weights per layer', first_linear_num_weights, conv_num_weights, second_linear_num_weights)\n",
    "print('n weights total', len(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an equivalent Neural network on Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tanh(x):\n",
    "    return jnp.tanh(x)\n",
    "\n",
    "def L2Norm(x):\n",
    "    x = x.T / (jnp.linalg.norm(x, ord=2, axis=1) + 1e-6)\n",
    "    return x.T\n",
    "\n",
    "def LinearLayer(w, b, x, print_weights=False):\n",
    "    w = w.reshape(len(b), -1)\n",
    "    out = jnp.dot(x, w.T) + b\n",
    "    if print_weights:\n",
    "        print('\\twi\\n\\t',w)\n",
    "        print('\\tbi\\n\\t',b)\n",
    "    return out\n",
    "    \n",
    "def ConvLayer(k, b, x, print_weights=False):\n",
    "    k = k.reshape(_hidden_c_out, _hidden_c_in, _kernel_size, _kernel_size)\n",
    "    out = lax.conv(x,\n",
    "                   k,\n",
    "                   (1,1),\n",
    "                   'SAME')\n",
    "    bias = jnp.einsum(\"c,bchw->bchw\", b, jnp.ones_like(out))\n",
    "    out = out + bias\n",
    "    if print_weights:\n",
    "        print('\\tki\\n\\t',k)\n",
    "        print('\\tbi\\n\\t',b)\n",
    "    return out\n",
    "\n",
    "def Flatten(x):\n",
    "    return x.reshape(_batch_size, -1)\n",
    "\n",
    "def Reshape(x):\n",
    "    return x.reshape(_batch_size, _hidden_c_in, _hidden_h, _hidden_w)\n",
    "\n",
    "def ResNet(x, *layers):\n",
    "    tmp_x = x\n",
    "    for layer in layers:\n",
    "        tmp_x = layer(tmp_x)\n",
    "    return tmp_x + x\n",
    "\n",
    "def jax_model(weights, x, print_weights=False, return_feature_maps=False):\n",
    "    # split the weights array\n",
    "    linear1_weights = weights[ : first_linear_num_weights]\n",
    "    w1 = linear1_weights[ : _input_size*_hidden_size]\n",
    "    b1 = linear1_weights[_input_size*_hidden_size : ]\n",
    "    LL1 = lambda x : LinearLayer(w1, b1, x, print_weights=print_weights)\n",
    "\n",
    "    conv_weights = weights[first_linear_num_weights : -second_linear_num_weights]\n",
    "    conv_k = conv_weights[ : -_hidden_c_out]\n",
    "    conv_b = conv_weights[-_hidden_c_out : ]\n",
    "    assert len(conv_k) == _hidden_c_in * _hidden_c_out * _kernel_size * _kernel_size and len(conv_b) == _hidden_c_out\n",
    "    CL = lambda x : ConvLayer(conv_k, conv_b, x, print_weights=print_weights)\n",
    "\n",
    "    linear2_weights = weights[-second_linear_num_weights : ]\n",
    "    w2 = linear2_weights[ : _hidden_c_out * _hidden_h * _hidden_w *_output_size]\n",
    "    b2 = linear2_weights[_hidden_c_out * _hidden_h * _hidden_w * _output_size : ]\n",
    "    LL2 = lambda x : LinearLayer(w2, b2, x, print_weights=print_weights)\n",
    "\n",
    "    RN = lambda x : ResNet(x, Reshape, CL, Flatten)\n",
    "\n",
    "    fm = [x]\n",
    "    for layer in [LL1, Tanh, RN, Tanh, LL2]: #, L2Norm]:\n",
    "        x = layer(x)\n",
    "        fm.append(x)\n",
    "    if return_feature_maps:\n",
    "        return x, fm\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that outputs are the same for random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_inputs_amount = 100\n",
    "\n",
    "for _ in range(random_inputs_amount):\n",
    "    x = torch.randn(_batch_size, _input_size)\n",
    "    jax_x = jnp.array(x.numpy())\n",
    "\n",
    "    y = stoch_model(x)\n",
    "    jax_y,fm = jax_model(weights, jax_x, return_feature_maps=True)\n",
    "\n",
    "    #print([np.max(abs(fs.detach().numpy() - np.array(fj))) for fs,fj in zip(stoch_model.feature_maps, fm)])\n",
    "    assert max([np.max(abs(fs.detach().numpy() - np.array(fj))) for fs,fj in zip(stoch_model.feature_maps, fm)]) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check correctness wrt weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that Vector-Jacobian products are the same for random inputs and random vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_inputs_amount = 10\n",
    "random_vectors_amount = 10\n",
    "\n",
    "for _ in range(random_inputs_amount):\n",
    "    x = torch.randn(_batch_size, _input_size)\n",
    "    jax_x = jnp.array(x.numpy())\n",
    "\n",
    "    jax_y_by_vjp, vjp_fun = vjp(lambda w: jax_model(w, jax_x), weights)\n",
    "\n",
    "    for _ in range(random_vectors_amount):\n",
    "        vector = torch.randn(_batch_size, _output_size)\n",
    "        jax_vector = jnp.array(vector.numpy())\n",
    "\n",
    "        #vector = torch.zeros((_batch_size, _output_size))\n",
    "        #vector[0,0] = 1\n",
    "        #jax_vector = jnp.zeros((_batch_size, _output_size))\n",
    "        #jax_vector = jax_vector.at[0,0].set(1)\n",
    "\n",
    "        jax_vj = vjp_fun(jax_vector)\n",
    "        stoch_vj = stoch_model._vjp(x, None, vector, wrt='weight')\n",
    "\n",
    "        difference = np.array(jax_vj) - stoch_vj.detach().numpy()\n",
    "        assert np.max(abs(difference)) < 1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix one input x and compute the Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian shape (2, 326)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(_batch_size, _input_size)\n",
    "jax_x = jnp.array(x.numpy())\n",
    "jax_y_by_vjp, vjp_fun = vjp(lambda w: jax_model(w, jax_x), weights)\n",
    "\n",
    "\n",
    "# define the identity matrix, for each batch element\n",
    "identity = []\n",
    "for i in range(_output_size):\n",
    "    e_i = jnp.zeros((_batch_size, _output_size))\n",
    "    for b in range(_batch_size):\n",
    "        e_i = e_i.at[b,i].set(1)\n",
    "    identity.append(e_i)\n",
    "\n",
    "J_by_jax = []\n",
    "for e_i in identity:\n",
    "    v_i = vjp_fun(e_i)\n",
    "    J_by_jax.append(v_i[0])\n",
    "\n",
    "J_by_jax = np.array(J_by_jax)\n",
    "print('Jacobian shape', J_by_jax.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a random matrix and backpropagate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check block diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "non reforwarding 4\n",
      "(98, 98) (98, 98)\n",
      "2.9802322e-07\n",
      "1.9350502 1.9350502\n",
      "(84, 84) (1, 84, 84)\n",
      "5.9604645e-08\n",
      "0.32389998 0.32389998\n",
      "(144, 144) (144, 144)\n",
      "1.1175871e-08\n",
      "0.04994989 0.049949884\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.randn(_batch_size, _output_size, _output_size)\n",
    "jax_matrix = matrix.numpy()\n",
    "\n",
    "jmj_by_jax = np.einsum(\"ji,bjk,kq->biq\", J_by_jax, jax_matrix, J_by_jax)\n",
    "jmj_by_stoch = stoch_model._jTmjp(x, None, matrix, wrt='weight')\n",
    "\n",
    "blocks_by_jax = [jmj_by_jax[0][:first_linear_num_weights, :first_linear_num_weights], \n",
    "                 jmj_by_jax[0][first_linear_num_weights: -second_linear_num_weights, first_linear_num_weights: -second_linear_num_weights], \n",
    "                 jmj_by_jax[0][-second_linear_num_weights:, -second_linear_num_weights:]\n",
    "]   \n",
    "\n",
    "for block in range(2,-1,-1):\n",
    "      print(blocks_by_jax[block].shape , jmj_by_stoch[block][0].detach().numpy().shape)\n",
    "      difference = blocks_by_jax[block] - jmj_by_stoch[block][0].detach().numpy()\n",
    "      print(np.max(abs(difference)))\n",
    "      print(np.max(abs(blocks_by_jax[block])),\n",
    "            np.max(abs(jmj_by_stoch[block][0].detach().numpy())))\n",
    "      assert np.max(abs(difference)) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check exact diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "non reforwarding 4\n",
      "(326,) (326,)\n",
      "[ 1.1175871e-08  0.0000000e+00  3.7252903e-09 -1.8626451e-08\n",
      " -1.1641532e-10  0.0000000e+00  7.4505806e-09  0.0000000e+00\n",
      "  7.4505806e-09 -2.2351742e-08  0.0000000e+00  0.0000000e+00\n",
      " -1.4901161e-08  0.0000000e+00  7.4505806e-09  2.9802322e-08\n",
      "  5.5879354e-09  0.0000000e+00  1.1175871e-08 -2.2351742e-08\n",
      "  1.3038516e-08  1.3969839e-09  0.0000000e+00  4.4703484e-08\n",
      " -2.9802322e-08  2.7939677e-09  7.4505806e-09  0.0000000e+00\n",
      " -1.4901161e-08  0.0000000e+00  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.randn(_batch_size, _output_size, _output_size)\n",
    "jax_matrix = matrix.numpy()\n",
    "\n",
    "jmj_by_jax = np.einsum(\"ji,bjk,ki->bi\", J_by_jax, jax_matrix, J_by_jax)\n",
    "jmj_by_stoch = stoch_model._jTmjp(x, None, matrix, wrt='weight', to_diag=True)\n",
    "\n",
    "print(jmj_by_jax[0].shape , jmj_by_stoch[0].detach().numpy().shape)\n",
    "difference = jmj_by_jax[0] - jmj_by_stoch[0].detach().numpy()\n",
    "print(difference[-31:])\n",
    "assert np.max(abs(difference)) < 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check correctness wrt the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that Vector-Jacobian products are the same for random inputs and random vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_inputs_amount = 10\n",
    "random_vectors_amount = 10\n",
    "\n",
    "for _ in range(random_inputs_amount):\n",
    "    x = torch.randn(_batch_size, _input_size)\n",
    "    jax_x = jnp.array(x.numpy())\n",
    "\n",
    "    jax_y_by_vjp, vjp_fun = vjp(lambda data: jax_model(weights, data), jax_x)\n",
    "\n",
    "    for _ in range(random_vectors_amount):\n",
    "        vector = torch.randn(_batch_size, _output_size)\n",
    "        jax_vector = jnp.array(vector.numpy())\n",
    "\n",
    "        #vector = torch.zeros((_batch_size, _output_size))\n",
    "        #vector[0,0] = 1\n",
    "        #jax_vector = jnp.zeros((_batch_size, _output_size))\n",
    "        #jax_vector = jax_vector.at[0,0].set(1)\n",
    "\n",
    "        jax_vj = vjp_fun(jax_vector)\n",
    "        stoch_vj = stoch_model._vjp(x, None, vector, wrt='input')\n",
    "\n",
    "        difference = np.array(jax_vj) - stoch_vj.detach().numpy()\n",
    "        assert np.max(abs(difference)) < 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix one input x and compute the Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian shape (2, 2)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(_batch_size, _input_size)\n",
    "jax_x = jnp.array(x.numpy())\n",
    "jax_y_by_vjp, vjp_fun = vjp(lambda data: jax_model(weights, data), jax_x)\n",
    "\n",
    "\n",
    "# define the identity matrix, for each batch element\n",
    "identity = []\n",
    "for i in range(_output_size):\n",
    "    e_i = jnp.zeros((_batch_size, _output_size))\n",
    "    for b in range(_batch_size):\n",
    "        e_i = e_i.at[b,i].set(1)\n",
    "    identity.append(e_i)\n",
    "\n",
    "J_by_jax = []\n",
    "for e_i in identity:\n",
    "    v_i = vjp_fun(e_i)\n",
    "    J_by_jax.append(v_i[0][0])\n",
    "\n",
    "J_by_jax = np.array(J_by_jax)\n",
    "print('Jacobian shape', J_by_jax.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a random matrix and backpropagate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check full case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "(2, 2) (2, 2)\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.randn(_batch_size, _output_size, _output_size)\n",
    "jax_matrix = matrix.numpy()\n",
    "\n",
    "jmj_by_jax = np.einsum(\"ji,bjk,kq->biq\", J_by_jax, jax_matrix, J_by_jax)\n",
    "jmj_by_stoch = stoch_model._jTmjp(x, None, matrix, wrt='input')\n",
    "\n",
    "print(jmj_by_jax[0].shape , jmj_by_stoch[0].detach().numpy().shape)\n",
    "difference = jmj_by_jax - jmj_by_stoch.detach().numpy()\n",
    "assert np.max(abs(difference)) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check exact diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reforwarding 6\n",
      "non reforwarding 4\n",
      "(2,) (2,)\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.randn(_batch_size, _output_size, _output_size)\n",
    "jax_matrix = matrix.numpy()\n",
    "\n",
    "jmj_by_jax = np.einsum(\"ji,bjk,ki->bi\", J_by_jax, jax_matrix, J_by_jax)\n",
    "jmj_by_stoch = stoch_model._jTmjp(x, None, matrix, wrt='input', to_diag=True)\n",
    "\n",
    "print(jmj_by_jax[0].shape , jmj_by_stoch[0].detach().numpy().shape)\n",
    "difference = jmj_by_jax[0] - jmj_by_stoch[0].detach().numpy()\n",
    "assert np.max(abs(difference)) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix a pair of inputs (x1, x2) and compute the Jacobians (wrt weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian shape (2, 326)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(_batch_size, _input_size)\n",
    "x2 = torch.randn(_batch_size, _input_size)\n",
    "jax_x1 = jnp.array(x1.numpy())\n",
    "jax_x2 = jnp.array(x2.numpy())\n",
    "jax_y1_by_vjp, vjp_fun1 = vjp(lambda w: jax_model(w, jax_x1), weights)\n",
    "jax_y2_by_vjp, vjp_fun2 = vjp(lambda w: jax_model(w, jax_x2), weights)\n",
    "\n",
    "\n",
    "# define the identity matrix, for each batch element\n",
    "identity = []\n",
    "for i in range(_output_size):\n",
    "    e_i = jnp.zeros((_batch_size, _output_size))\n",
    "    for b in range(_batch_size):\n",
    "        e_i = e_i.at[b,i].set(1)\n",
    "    identity.append(e_i)\n",
    "\n",
    "J1_by_jax = []\n",
    "J2_by_jax = []\n",
    "for e_i in identity:\n",
    "    v1_i = vjp_fun1(e_i)\n",
    "    v2_i = vjp_fun2(e_i)\n",
    "    J1_by_jax.append(v1_i[0])\n",
    "    J2_by_jax.append(v2_i[0])\n",
    "\n",
    "J1_by_jax = np.array(J1_by_jax)\n",
    "J2_by_jax = np.array(J2_by_jax)\n",
    "assert J1_by_jax.shape == J2_by_jax.shape\n",
    "print('Jacobian shape', J1_by_jax.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a random matrix and backpropagate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check block diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 144) (144, 144)\n",
      "5.9604645e-08\n",
      "(84, 84) (84, 84)\n",
      "1.3411045e-07\n",
      "(98, 98) (98, 98)\n",
      "1.1920929e-06\n"
     ]
    }
   ],
   "source": [
    "matrixes = tuple(torch.randn(_batch_size, _output_size, _output_size) for _ in range(3))\n",
    "jax_matrixes = tuple(matrix.numpy() for matrix in matrixes)\n",
    " \n",
    "jmj_by_jax = np.einsum(\"ji,bjk,kq->biq\", J1_by_jax, jax_matrixes[0], J1_by_jax) \\\n",
    "                - np.einsum(\"ji,bjk,kq->biq\", J1_by_jax, jax_matrixes[1], J2_by_jax) \\\n",
    "                - np.einsum(\"ji,bjk,kq->bqi\", J1_by_jax, jax_matrixes[1], J2_by_jax) \\\n",
    "                + np.einsum(\"ji,bjk,kq->biq\", J2_by_jax, jax_matrixes[2], J2_by_jax)\n",
    "jmj_by_stoch = stoch_model._jTmjp_batch2(x1, x2, None, None, matrixes, wrt='weight')\n",
    "jmj_by_stoch = [matrixes[0] - matrixes[1] - matrixes[1].transpose(-2,-1) + matrixes[2] for matrixes in jmj_by_stoch]\n",
    "\n",
    "blocks_by_jax = [jmj_by_jax[0][:first_linear_num_weights, :first_linear_num_weights], \n",
    "                 jmj_by_jax[0][first_linear_num_weights: -second_linear_num_weights, first_linear_num_weights: -second_linear_num_weights], \n",
    "                 jmj_by_jax[0][-second_linear_num_weights:, -second_linear_num_weights:]\n",
    "]   \n",
    "\n",
    "for block in range(3):\n",
    "    print(blocks_by_jax[block].shape , jmj_by_stoch[block][0].detach().numpy().shape)\n",
    "    difference = blocks_by_jax[block] - jmj_by_stoch[block][0].detach().numpy()\n",
    "    print(np.max(abs(difference)))\n",
    "    assert np.max(abs(difference)) < 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jmj_by_stoch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check exact diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(326,) (326,)\n",
      "4.7683716e-07\n"
     ]
    }
   ],
   "source": [
    "matrixes = tuple(torch.randn(_batch_size, _output_size, _output_size) for _ in range(3))\n",
    "jax_matrixes = tuple(matrix.numpy() for matrix in matrixes)\n",
    " \n",
    "jmj_by_jax = np.einsum(\"ji,bjk,ki->bi\", J1_by_jax, jax_matrixes[0], J1_by_jax) \\\n",
    "                - 2 * np.einsum(\"ji,bjk,ki->bi\", J1_by_jax, jax_matrixes[1], J2_by_jax) \\\n",
    "                + np.einsum(\"ji,bjk,ki->bi\", J2_by_jax, jax_matrixes[2], J2_by_jax)\n",
    "jmj_by_stoch = stoch_model._jTmjp_batch2(x1, x2, None, None, matrixes, wrt='weight', to_diag=True)\n",
    "jmj_by_stoch = [matrixes[0] - 2 * matrixes[1] + matrixes[2] for matrixes in jmj_by_stoch]\n",
    "jmj_by_stoch = torch.cat(jmj_by_stoch, dim=1)\n",
    "\n",
    "print(jmj_by_jax[0].shape , jmj_by_stoch[0].detach().numpy().shape)\n",
    "difference = jmj_by_jax[0] - jmj_by_stoch[0].detach().numpy()\n",
    "print(np.max(abs(difference)))\n",
    "assert np.max(abs(difference)) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix a pair of inputs (x1, x2) and compute the Jacobians (wrt weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian shape (2, 2)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(_batch_size, _input_size)\n",
    "x2 = torch.randn(_batch_size, _input_size)\n",
    "jax_x1 = jnp.array(x1.numpy())\n",
    "jax_x2 = jnp.array(x2.numpy())\n",
    "jax_y1_by_vjp, vjp_fun1 = vjp(lambda data: jax_model(weights, data), jax_x1)\n",
    "jax_y2_by_vjp, vjp_fun2 = vjp(lambda data: jax_model(weights, data), jax_x2)\n",
    "\n",
    "\n",
    "# define the identity matrix, for each batch element\n",
    "identity = []\n",
    "for i in range(_output_size):\n",
    "    e_i = jnp.zeros((_batch_size, _output_size))\n",
    "    for b in range(_batch_size):\n",
    "        e_i = e_i.at[b,i].set(1)\n",
    "    identity.append(e_i)\n",
    "\n",
    "J1_by_jax = []\n",
    "J2_by_jax = []\n",
    "for e_i in identity:\n",
    "    v1_i = vjp_fun1(e_i)\n",
    "    v2_i = vjp_fun2(e_i)\n",
    "    J1_by_jax.append(v1_i[0][0])\n",
    "    J2_by_jax.append(v2_i[0][0])\n",
    "\n",
    "J1_by_jax = np.array(J1_by_jax)\n",
    "J2_by_jax = np.array(J2_by_jax)\n",
    "assert J1_by_jax.shape == J2_by_jax.shape\n",
    "print('Jacobian shape', J1_by_jax.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a random matrix and backpropagate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check full case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2) (2, 2)\n"
     ]
    }
   ],
   "source": [
    "matrixes = tuple(torch.randn(_batch_size, _output_size, _output_size) for _ in range(3))\n",
    "jax_matrixes = tuple(matrix.numpy() for matrix in matrixes)\n",
    " \n",
    "jmj_by_jax = np.einsum(\"ji,bjk,kq->biq\", J1_by_jax, jax_matrixes[0], J1_by_jax) \\\n",
    "                - np.einsum(\"ji,bjk,kq->biq\", J1_by_jax, jax_matrixes[1], J2_by_jax) \\\n",
    "                - np.einsum(\"ji,bjk,kq->bqi\", J1_by_jax, jax_matrixes[1], J2_by_jax) \\\n",
    "                + np.einsum(\"ji,bjk,kq->biq\", J2_by_jax, jax_matrixes[2], J2_by_jax)\n",
    "jmj_by_stoch = stoch_model._jTmjp_batch2(x1, x2, None, None, matrixes, wrt='input')\n",
    "jmj_by_stoch = jmj_by_stoch[0] - jmj_by_stoch[1] - jmj_by_stoch[1].transpose(-2,-1) + jmj_by_stoch[2]\n",
    "\n",
    "\n",
    "print(jmj_by_jax[0].shape , jmj_by_stoch[0].detach().numpy().shape)\n",
    "difference = jmj_by_jax - jmj_by_stoch.detach().numpy()\n",
    "assert np.max(abs(difference)) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check exact diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) (2,)\n"
     ]
    }
   ],
   "source": [
    "matrixes = tuple(torch.randn(_batch_size, _output_size, _output_size) for _ in range(3))\n",
    "jax_matrixes = tuple(matrix.numpy() for matrix in matrixes)\n",
    " \n",
    "jmj_by_jax = np.einsum(\"ji,bjk,ki->bi\", J1_by_jax, jax_matrixes[0], J1_by_jax) \\\n",
    "                - 2* np.einsum(\"ji,bjk,ki->bi\", J1_by_jax, jax_matrixes[1], J2_by_jax) \\\n",
    "                + np.einsum(\"ji,bjk,ki->bi\", J2_by_jax, jax_matrixes[2], J2_by_jax)\n",
    "jmj_by_stoch = stoch_model._jTmjp_batch2(x1, x2, None, None, matrixes, wrt='input', to_diag=True)\n",
    "jmj_by_stoch = jmj_by_stoch[0] - 2 * jmj_by_stoch[1] + jmj_by_stoch[2]\n",
    "\n",
    "\n",
    "print(jmj_by_jax[0].shape , jmj_by_stoch[0].detach().numpy().shape)\n",
    "difference = jmj_by_jax - jmj_by_stoch.detach().numpy()\n",
    "assert np.max(abs(difference)) < 1e-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('laplace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "477c387996a084153fd8ef9c4e5432322bdf7a5fc9ad0e2a4c514862f58a6dd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
